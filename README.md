````markdown
## 1ï¸âƒ£ Recommended Repository Structure ğŸ“

```text
etl-toll-data-airflow/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ETL_toll_data.py
â”œâ”€â”€ staging/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ dag_args.jpg
â”‚   â”œâ”€â”€ dag_definition.jpg
â”‚   â”œâ”€â”€ unzip_data.jpg
â”‚   â”œâ”€â”€ extract_data_from_csv.jpg
â”‚   â”œâ”€â”€ extract_data_from_tsv.jpg
â”‚   â”œâ”€â”€ extract_data_from_fixed_width.jpg
â”‚   â”œâ”€â”€ consolidate_data.jpg
â”‚   â”œâ”€â”€ transform.jpg
â”‚   â”œâ”€â”€ task_pipeline.jpg
â”‚   â”œâ”€â”€ submit_dag.jpg
â”‚   â”œâ”€â”€ unpause_trigger_dag.jpg
â”‚   â”œâ”€â”€ dag_tasks.jpg
â”‚   â””â”€â”€ dag_runs.jpg
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
````

---

# ETL Toll Data Pipeline (Apache Airflow) ğŸš¦

**Author:** Denis Naumov
**Role:** Data Engineer
**Stack:** Apache Airflow, Bash, Linux, CSV / TSV / Fixed-width files

---

## ğŸ“Œ Project Overview

This project implements a production-style **ETL pipeline using Apache Airflow** to consolidate heterogeneous toll road traffic data from multiple toll operators into a single structured dataset.

Each operator provides data in a different format:

* CSV
* TSV
* Fixed-width text file

The pipeline extracts, transforms, and consolidates the data into a unified staging dataset for downstream analytics.

---

## ğŸ¯ Objectives

The Airflow DAG performs the following steps:

1. Extract data from a CSV file
2. Extract data from a TSV file
3. Extract data from a fixed-width file
4. Consolidate extracted datasets
5. Transform vehicle type values to uppercase
6. Store the final output in a staging area

---

## ğŸ§± Architecture

```text
Raw Data (tgz)
   â†“
Unzip
   â†“
Extract (CSV | TSV | Fixed-width)
   â†“
Consolidate
   â†“
Transform
   â†“
Staging Output
```

---

## âš™ï¸ DAG Details

* **DAG ID:** `ETL_toll_data`
* **Schedule:** Daily
* **Operator Type:** `BashOperator`
* **Retry Policy:**

  * Retries: 1
  * Retry Delay: 5 minutes
* **Email Alerts:** Enabled for failure and retry

---

## ğŸ—‚ï¸ Data Flow

| Step                | Output File            |
| ------------------- | ---------------------- |
| CSV Extract         | `csv_data.csv`         |
| TSV Extract         | `tsv_data.csv`         |
| Fixed-width Extract | `fixed_width_data.csv` |
| Consolidation       | `extracted_data.csv`   |
| Transformation      | `transformed_data.csv` |

---

## ğŸ“ Staging Directory

```bash
/home/project/airflow/dags/finalassignment/staging
```

This directory stores the final transformed dataset.

---

## ğŸš€ How to Run

1. Place the DAG file in your Airflow `dags/` directory
2. Start Airflow:

   ```bash
   airflow webserver
   airflow scheduler
   ```
3. Unpause the DAG:

   ```bash
   airflow dags unpause ETL_toll_data
   ```
4. Trigger the DAG and monitor execution

---

## ğŸ–¼ï¸ Screenshots

All required execution and validation screenshots are stored in the `screenshots/` directory:

* DAG definition
* Task pipeline
* Task execution
* Successful DAG runs

---

## ğŸ§  Engineering Notes

* Uses Linux-native utilities (`cut`, `paste`, `tr`) for efficient ETL processing
* Designed for clarity, reproducibility, and operational simplicity
* Follows Apache Airflow best practices for DAG design and task dependencies

---

## ğŸ“œ License

MIT License

Â© 2025 Denis Naumov

```
```
